model:
  type: vqgan
  mode: None         # tokenizer, transformer, or None
  in_channels: 3          
  ch: 128                  # base number of feature channels
  ch_mult: [1,2]      # channel multiplier per resolution level
  num_res_blocks: 1       # residual blocks per level
  z_channels: 64   
  num_codebook_vectors: 512
  beta:  0.25
  decay: 0.99
  disc_start: 10000             # When to start the discriminator
  disc_factor: 0.2              # Discriminator loss factor
  rec_loss_factor: 1.0          # Weighting factor for reconstruction loss
  perceptual_loss_factor: 1.0   # Weighting factor for perceptual loss
  checkpoint_dir: &ckpt_dir "/nlp/scr/asap7772/checkpoints/vqgan_ffhq"

data:
  dataset: ffhq
  path: '/nlp/scr/asap7772/data/images1024x1024' 
  image_size: 256
  batch_size: 256
  num_workers: 4
  shuffle: true

training:
  epochs: 100
  learning_rate: 0.0002
  
  # Checkpointing
  checkpoint_dir: *ckpt_dir  # <- reuse the anchor
  save_interval: 10  # Save every N epochs
  
  # Logging
  sample_interval: 500  # Sample reconstructions every N steps


wandb:
  project: "generative-models"
  name: "vqgan_ffhq"
  tags: ["vqgan", "ffhq"]
  log_model: false

seed: 40