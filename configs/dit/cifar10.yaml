model:
  type: dit  # diffusion transformer
  img_size: 32
  patch_size: 4
  in_channels: 3
  hidden_size: 384  # Smaller for CIFAR-10
  depth: 12
  num_heads: 6
  mlp_ratio: 4.0
  dropout: 0.0
  timesteps: 1000
  beta_schedule: linear  # linear or cosine

data:
  dataset: cifar10
  path: "/home/groups/swl1/lhy/data/cifar10"  # Set your CIFAR-10 data path here
  batch_size: 128
  num_workers: 4
  shuffle: true

training:
  epochs: 500
  learning_rate: 0.0001 #1e-4
  weight_decay: 0.0
  grad_clip: 1.0
  
  # Optimizer
  optimizer: adamw
  
  
  # EMA settings
  use_ema: true
  ema_decay: 0.9999
  ema_inv_gamma: 1.0
  ema_power: 0.6667  # 2/3
  
  # Sampling settings
  sampling_scheduler: ddim  # ddpm or ddim (DDIM is faster)
  num_inference_steps: 50
  
  # Checkpointing
  checkpoint_dir: "/oak/stanford/groups/swl1/lhy/checkpoints/generative_models/dit_cifar10"
  save_interval: 25
  
  # Logging
  log_interval: 100
  sample_interval: 1000
  num_samples: 16

wandb:
  project: "generative-models"
  entity: null
  name: "dit_cifar10"
  tags: ["dit", "diffusion-transformer", "cifar10"]
  log_model: false

seed: 42