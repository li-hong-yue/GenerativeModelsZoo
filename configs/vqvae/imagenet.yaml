model:
  type: vqvae
  image_size: 256  # ImageNet is typically 256x256 or 224x224
  in_channels: 3
  ch: 128                  # base number of feature channels
  ch_mult: [1, 2, 4, 8]      # channel multiplier per resolution level
  num_res_blocks: 2       # residual blocks per level
  z_channels: 128    
  num_codebook_vectors: 4096
  beta:  0.25

data:
  dataset: imagenet
  path: "/scratch/groups/swl1/lhy/imagenet"  # Set your ImageNet data path here (e.g., /path/to/imagenet)
  batch_size: 32  # Smaller batch size due to larger images
  num_workers: 8
  shuffle: true
  image_size: 256  # Preprocessing size

training:
  epochs: 100
  learning_rate: 0.00005 #1e-4  # Lower LR for larger dataset
  weight_decay: 0.00001 # 1e-5
  
  grad_clip: 1.0
  optimizer: adamw  # AdamW often works better for ImageNet
  
  # Checkpointing
  checkpoint_dir: "/oak/stanford/groups/swl1/lhy/checkpoints/generative_models/vqvae_imagenet"
  save_interval: 5  # Save more frequently
  
  # Logging
  log_interval: 50  # Log more frequently
  sample_interval: 500
  num_samples: 8  # Fewer samples due to larger image size

wandb:
  project: "generative-models"
  entity: null  # Set your wandb entity/username
  name: "vqvae_imagenet_256"
  tags: ["vqvae", "imagenet", "256x256"]
  log_model: false

seed: 42